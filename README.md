# ChatGPT-Seminar

In the rapidly evolving field of Natural Language Processing (NLP), transformer-based language models have made significant advances. However, these models predominantly focus on high-resource languages, thereby exacerbating the digital divide and linguistic inequality. To address the challenge of improving the performance of pre-trained language models in low-resource languages (LRLs) without access to an annotated instruction-following dataset in the target language, this paper proposes a data augmentation method using machine translation. Our approach aims to expand the pool of languages that a pre-trained model can understand, while fine-tuning it for instruction-following tasks. We demonstrate the effectiveness of our approach by adapting the Bloom-1b1 language model to German using an automatically translated dataset derived from the Stanford Alpaca project. This experiment, which involves data augmentation via machine translation, serves to verify our proposed method. The performance of the adapted model was evaluated by human evaluation, comparing it a comparable language model. Despite the limitations of the translated dataset, the fine-tuning constraints and the evaluation process, our results show promise for bridging the linguistic divide and promoting digital equality. Future work should focus on improving translation quality, exploring alternative architectures and hyperparameters, and conducting more comprehensive evaluations to validate and extend the results of this study.
